{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdb6316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/JHA/mb5157/CONDA/envs/graphmert/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os, csv\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets, load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/scratch/gpfs/JHA/mb5157/tokenizers/biomedbert_fast_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95b2ca-2791-415b-8cd0-77646b99b32f",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74ae1a-23f1-4f9c-bd8b-ecbedd613082",
   "metadata": {},
   "source": [
    "### 1. Unite dataset chunks after combining tokens\n",
    "\n",
    "You need this one if you launched combine_tails.py in multiple parallel jobs. If you didn't that still should work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894ee810-982e-4d93-8368-a512c4fbcedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite_output(dataset_path, original_dataset_size, chunk_size, llm_name, start_idx=0):\n",
    "    \"\"\"\n",
    "    original_dataset_size -- how many records in the original dataset;\n",
    "    it's also in the filename in the folder -- the largest end index\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(dataset_path, f'{llm_name}_all')\n",
    "    end_idx = start_idx\n",
    "\n",
    "    datasets = []\n",
    "    while end_idx < original_dataset_size:\n",
    "        end_idx = min(start_idx + chunk_size, original_dataset_size)\n",
    "        path = os.path.join(dataset_path, f\"{llm_name}_{start_idx}-{end_idx}\")\n",
    "        dataset = load_from_disk(path)            \n",
    "        print(f'loaded from {path}')\n",
    "        datasets.append(dataset)\n",
    "        start_idx += chunk_size\n",
    "    \n",
    "    united_dataset = concatenate_datasets(datasets)\n",
    "    united_dataset.save_to_disk(output_path)\n",
    "    print('saved to', output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2aa32b-2b2a-4de4-a190-153d6295f965",
   "metadata": {},
   "source": [
    "#### Set your own variables in the cell below\n",
    "\n",
    "They are used to buld the correct path to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80869fa-9db3-46a6-9580-dfb78636f21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from ../../outputs/test/span/rel29/bs256_lr_0.0001/predictions/top_10/qwen3-1.7b_0-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 19917.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ../../outputs/test/span/rel29/bs256_lr_0.0001/predictions/top_10/qwen3-1.7b_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prefix = '../..'\n",
    "# specify path to trained graphmert model\n",
    "model_path = 'outputs/test/span/rel29/bs256_lr_0.0001' \n",
    "top_k = 10\n",
    "dataset_path = os.path.join(prefix, model_path, f'predictions/top_{top_k}')\n",
    "llm_name = 'qwen3-1.7b' # llm used to combine tokens\n",
    "original_dataset_size = 100\n",
    "\n",
    "saved_to_path = None # to notice if the function unite_output didn't work after re-run\n",
    "saved_to_path = unite_output(dataset_path, original_dataset_size, chunk_size=10000, llm_name = llm_name, start_idx=0)\n",
    "united_dataset = load_from_disk(saved_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9140a9d-0b82-40a6-993b-011865168aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have chunks, you can simly load your dataset here\n",
    "# saved_to_path = ...\n",
    "# united_dataset = load_from_disk(saved_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137e39d-08f5-4b0f-9c37-368729a92301",
   "metadata": {},
   "source": [
    "### 2. Remove hallicinated tails: not from the prediction\n",
    "\n",
    "Use filter_tails func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ae9842-00ee-452e-8eab-5d8367ebb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tails(example):\n",
    "    \"\"\"aggressive filtering: remove all tails with a token not in GLM prediction\"\"\"\n",
    "    preds = set(example['predictions'].split())\n",
    "    kept, removed, removed_missing = [], [], []\n",
    "    for tail in example['tails']:\n",
    "        tl = tail.lower()\n",
    "        toks = tokenizer.tokenize(tl)\n",
    "        if all(tok in preds for tok in toks):\n",
    "            kept.append(tl)\n",
    "        else:\n",
    "            removed.append(tl)\n",
    "            removed_missing.append([tok for tok in toks if tok not in preds])\n",
    "    return {\n",
    "        'tails': kept,\n",
    "        'removed_tails': removed, # the actual tail strings that were filtered out\n",
    "        'removed_missing_tokens': removed_missing, # tokens that triggered the removal\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0af92dc-49a7-453c-9c92-ec80cd64db93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tails kept: 0\n",
      "tails removed: 0\n",
      "tails total (before filtering): 0\n",
      "records in the dataset: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 9965.32 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to outputs/test/span/rel29/bs256_lr_0.0001/predictions/top_10/qwen3-32b_subset_100_0-100_clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_dataset = united_dataset.map(\n",
    "    filter_tails, num_proc=20, desc=\"filter tails\"\n",
    ")\n",
    "\n",
    "# ---- totals ----\n",
    "kept_total    = sum(len(x) for x in clean_dataset['tails'])\n",
    "removed_total = sum(len(x) for x in clean_dataset['removed_tails'])\n",
    "total_before  = kept_total + removed_total\n",
    "\n",
    "print(f\"tails kept: {kept_total}\")\n",
    "print(f\"tails removed: {removed_total}\")\n",
    "print(f\"tails total (before filtering): {total_before}\")\n",
    "print(f'records in the dataset: {len(clean_dataset)}')\n",
    "\n",
    "clean_output_path = f'{saved_to_path}_clean'\n",
    "clean_dataset.select_columns(['id', 'input_ids', 'head', 'relation', 'predictions', 'tails']).save_to_disk(clean_output_path)\n",
    "print(f'saved to {clean_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f830d8f-21e1-4c7e-9911-68db23cd5633",
   "metadata": {},
   "source": [
    "### Functions below are experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36919f2f-d393-4639-a9a6-6bb7f61268a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tails_relaxed(example):\n",
    "    \"\"\"relaxed filtering: allows tail tokens that are from the sequence\"\"\"\n",
    "    preds = set(example['predictions'].split())\n",
    "    kept, removed, removed_missing = [], [], []\n",
    "    for tail in example['tails']:\n",
    "        tl = tail.lower()\n",
    "        toks = set(tokenizer.tokenize(tl))\n",
    "        # len_toks_before = len(toks)\n",
    "        toks -= preds\n",
    "        if len(toks) == 0:\n",
    "            kept.append(tl)\n",
    "        else:\n",
    "            seq = set(tokenizer.decode(example['input_ids'], skip_special_tokens=True))\n",
    "            toks -= seq\n",
    "            if len(toks) == 0: #or len_toks_before\n",
    "                kept.append(tl)\n",
    "            else:\n",
    "                removed.append(tl)\n",
    "                removed_missing.append(list(toks))\n",
    "    return {\n",
    "        'tails': kept,\n",
    "        'removed_tails': removed, # the actual tail strings that were filtered out\n",
    "        'removed_missing_tokens': removed_missing, # tokens that triggered the removal\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_tails_loose(example):\n",
    "    \"\"\"\n",
    "    loose filtering:\n",
    "    allows tail tokens that are from the sequence\n",
    "    and one token from LLM for tails longer than 3 tokens\n",
    "    \"\"\"\n",
    "    preds = set(example['predictions'].split())\n",
    "    kept, removed, removed_missing = [], [], []\n",
    "    for tail in example['tails']:\n",
    "        tl = tail.lower()\n",
    "        toks = set(tokenizer.tokenize(tl))\n",
    "        len_toks_before = len(toks)\n",
    "        toks -= preds\n",
    "        if len(toks) == 0:\n",
    "            kept.append(tl)\n",
    "        else:\n",
    "            seq = set(tokenizer.decode(example['input_ids'], skip_special_tokens=True))\n",
    "            toks -= seq\n",
    "            if len(toks) == 0 or (len(toks) == 1 and len_toks_before > 2):\n",
    "                kept.append(tl)\n",
    "            else:\n",
    "                removed.append(tl)\n",
    "                removed_missing.append(list(toks))\n",
    "    return {\n",
    "        'tails': kept,\n",
    "        'removed_tails': removed, # the actual tail strings that were filtered out\n",
    "        'removed_missing_tokens': removed_missing, # tokens that triggered the removal\n",
    "    }\n",
    "\n",
    "\n",
    "def write_to_log(dataset, log_path):\n",
    "    with open(log_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        header = ['row_index', 'id', 'removed_tail', 'missing_tokens']\n",
    "        w.writerow(header)\n",
    "        has_id = 'id' in dataset.column_names\n",
    "        for i, row in enumerate(dataset):\n",
    "            if not i % 10_000:\n",
    "                print(f'at line {i}')\n",
    "            rid = row['id'] if has_id else ''\n",
    "            for tail, missing in zip(row['removed_tails'], row['removed_missing_tokens']):\n",
    "                w.writerow([i, rid, tail, ' '.join(missing)])\n",
    "    print(f\"saved to {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b9b35-3c87-4185-b5ea-ddb3eaee689c",
   "metadata": {},
   "source": [
    "#### relaxed cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2390e349-2b2e-4541-9d83-14ffe8749576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter tails (num_proc=20): 100%|██████████| 1341137/1341137 [01:04<00:00, 20647.17 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tails kept: 1544558\n",
      "tails removed: 215530\n",
      "tails total (before filtering): 1760088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 1341137/1341137 [00:01<00:00, 718713.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ../outputs/350abstracts/qwen32/score0.55/span/rel29/bs256_lr_0.0004/predictions/top_20/qwen3-32b_all_clean_relaxed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_dataset_relaxed = united_dataset.map(\n",
    "    filter_tails_relaxed, num_proc=20, desc=\"filter tails\"\n",
    ")\n",
    "\n",
    "# ---- totals ----\n",
    "kept_total    = sum(len(x) for x in clean_dataset_relaxed['tails'])\n",
    "removed_total = sum(len(x) for x in clean_dataset_relaxed['removed_tails'])\n",
    "total_before  = kept_total + removed_total\n",
    "\n",
    "print(f\"tails kept: {kept_total}\")\n",
    "print(f\"tails removed: {removed_total}\")\n",
    "print(f\"tails total (before filtering): {total_before}\")\n",
    "\n",
    "clean_output_path = f'{saved_to_path}_clean_relaxed'\n",
    "clean_dataset_relaxed.select_columns(['id', 'input_ids', 'head', 'relation', 'predictions', 'tails']).save_to_disk(clean_output_path)\n",
    "print(f'saved to {clean_output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7deef-45bf-4a27-b27b-b572d58ccdb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# log_path = os.path.join(dataset_path, f'removed_tails_relaxed.csv')\n",
    "# write_to_log(clean_dataset_relaxed, log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2aa13-6c1b-4044-bd0d-a8086d6e2870",
   "metadata": {},
   "source": [
    "#### loose cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc1583d6-78f1-40b0-b051-3a72d1e9feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filter tails (num_proc=20): 100%|██████████| 1517900/1517900 [01:22<00:00, 18486.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tails kept: 4012197\n",
      "tails removed: 202083\n",
      "tails total (before filtering): 4214280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 1517900/1517900 [00:02<00:00, 672660.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ../outputs/350abstracts/qwen32/score0.55/ablations/seed_kg_size/llm_kg/span/rel23/bs256_lr_0.0004/predictions/top_20/qwen3-32b_all_clean_loose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_dataset_loose = united_dataset.map(\n",
    "    filter_tails_loose, num_proc=20, desc=\"filter tails\"\n",
    ")\n",
    "\n",
    "kept_total    = sum(len(x) for x in clean_dataset_loose['tails'])\n",
    "removed_total = sum(len(x) for x in clean_dataset_loose['removed_tails'])\n",
    "total_before  = kept_total + removed_total\n",
    "\n",
    "print(f\"tails kept: {kept_total}\")\n",
    "print(f\"tails removed: {removed_total}\")\n",
    "print(f\"tails total (before filtering): {total_before}\")\n",
    "\n",
    "clean_output_path = f'{saved_to_path}_clean_loose'\n",
    "clean_dataset_loose.select_columns(['id', 'input_ids', 'head', 'relation', 'predictions', 'tails']).save_to_disk(clean_output_path)\n",
    "print(f'saved to {clean_output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed6a13-fd16-477a-9648-187e09e0e748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join(dataset_path, f'removed_tails_loose.csv')\n",
    "write_to_log(clean_dataset_loose, log_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmert [~/scratch/gpfs/JHA/mb5157/CONDA/envs/graphmert/]",
   "language": "python",
   "name": "conda_graphmert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
