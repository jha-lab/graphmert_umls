{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f892edd-c65f-4d3c-9f4d-75d2b377ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, csv\n",
    "import shutil\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets, load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/scratch/gpfs/JHA/mb5157/tokenizers/biomedbert_fast_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0899ba-4e41-4f79-8c0e-2e3ce7a9ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(dataset_path, dataset_len, step, suffix):\n",
    "    paths = []\n",
    "    for i in range(0, dataset_len, step):\n",
    "        last = dataset_len if i + step > dataset_len else i + step\n",
    "        path = os.path.join(dataset_path, f'{suffix}_{i}-{last}')\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def unite_chunks(paths):\n",
    "    chunks = []\n",
    "    for path in paths:\n",
    "        chunk = load_from_disk(path)            \n",
    "        print(f'loaded from {path}')\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    united_dataset = concatenate_datasets(chunks)\n",
    "    return united_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e71ec1-dd01-46ab-8e98-1a380f8bafbf",
   "metadata": {},
   "source": [
    "## Unite FactScore chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdbe08-954d-4186-8da4-1d8fb35312bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your parameters here\n",
    "#=======\n",
    "dataset_path = '../outputs/350abstracts/qwen32/score0.55/span/rel29/bs256_lr_0.0004/predictions/top_20'\n",
    "datasets = []\n",
    "\n",
    "# accepted_* -- based on sequence + general knowledge;\n",
    "# accepted_seq_only -- based on sequence only\n",
    "suffix = 'accepted_seq_only_qwen3-32b'\n",
    "suffices = []\n",
    "\n",
    "# the length of the dataset before running factscore evaluation\n",
    "dataset_len = 109293\n",
    "# saving step: batch_size * num_batches\n",
    "step = 100000\n",
    "#=========\n",
    "chunk_paths = get_chunks(dataset_path, dataset_len, step, suffix)\n",
    "united_dataset = unite_chunks(chunk_paths)\n",
    "len_accepted = len(united_dataset)\n",
    "print('accepted:', len_accepted)\n",
    "\n",
    "factscore = round(len_accepted / dataset_len, 3)\n",
    "print(f'FactScore: {factscore}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162a572-2d36-4ab4-92e4-74e03b313dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save united chunks\n",
    "output_path = os.path.join(dataset_path, f'{suffix}_all')\n",
    "united_dataset.save_to_disk(output_path)\n",
    "print(f'saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badbe51-81fd-4b95-8d30-13dc469aa3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful: THIS REMOVES ALL CHUNKS\n",
    "# make sure you saved united_dataset before running this cell\n",
    "for path in chunk_paths:\n",
    "    shutil.rmtree(path)\n",
    "    print(f'removed {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8d57c-4f76-4bcd-b762-253cb72de3e8",
   "metadata": {},
   "source": [
    "## Unite ValidityScore chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1f32f-bccf-447d-9edc-b3f1cb47819f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "suffices = []\n",
    "\n",
    "# === set your parameters here\n",
    "dataset_path = '/projects/JHA/shared/dataset/qwen32b/alpha_0.55/span/cleaned_graphs'\n",
    "datasets = []\n",
    "suffix = 'validated_gemini-2.0-flash'\n",
    "suffices = []\n",
    "\n",
    "# the length of the dataset before running factscore evaluation\n",
    "#  # get this one from output log:\n",
    "# This job predicts tails for examples from .. to .. out of <dataset_len>\n",
    "# or just look at the last chunk name\n",
    "dataset_len = 139565\n",
    "# saving step = batch_size * num_batches\n",
    "step = 1000\n",
    "# =========\n",
    "\n",
    "validity_chunk_paths = get_chunks(dataset_path, dataset_len, step, suffix)\n",
    "united_dataset = unite_chunks(validity_chunk_paths)\n",
    "print('accepted:', len(united_dataset))\n",
    "\n",
    "united_dataset_val = concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165184dd-b25f-4d84-b71a-d1070fad2998",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(dataset_path, f'{suffix}_all')\n",
    "united_dataset.save_to_disk(output_path)\n",
    "print(f'saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3bfb6-47b4-4bab-9a27-1531bac2c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValidtyScore is the percentage of \"yes\" responses\n",
    "for key_word in ('yes', 'no', 'maybe', ''):\n",
    "    # \"\" -- includes missing values\n",
    "    dataset_keyword = united_dataset_val.filter(lambda ex: ex[\"response\"] == [key_word],\n",
    "                                 desc=f\"Keep only {key_word}\")\n",
    "    \n",
    "    print(len(dataset_keyword))\n",
    "    print(f'{key_word}:', round(len(dataset_keyword)/len(united_dataset), 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac491e50-e90f-49e7-b0cb-90abab0bfc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(dataset_path, f'{suffix}_all')\n",
    "united_dataset_val.save_to_disk(output_path)\n",
    "print(f'saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76052c80-b0ff-4639-a0be-85de9755fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful: THIS REMOVES ALL CHUNKS\n",
    "# make sure you saved united_dataset before running this cell\n",
    "for path in validity_chunk_paths:\n",
    "    shutil.rmtree(path)\n",
    "    print(f'removed {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396c352-d821-4a04-8ea6-5ba951a490c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful: THIS REMOVES ALL CHUNKS\n",
    "# make sure you saved united_dataset before running this cell\n",
    "for path in chunk_paths:\n",
    "    shutil.rmtree(path)\n",
    "    print(f'removed {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523fb71-371b-4a48-84cc-a5341928cd33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphmert",
   "language": "python",
   "name": "graphmert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
